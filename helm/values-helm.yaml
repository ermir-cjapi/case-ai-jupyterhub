# JupyterHub Helm Values - Kubernetes Deployment
# This is the same as jupyterhub_config.py but in YAML format for Kubernetes

proxy:
  # Generate with: openssl rand -hex 32
  secretToken: "a7c63b7d74e9628282eca95874fa7a9f69797f027512bb6d9f0636bb9689307e"
  service:
    type: NodePort  # Expose on fixed port for Cloudflare tunnel
    nodePorts:
      http: 30080  # Fixed port - update Cloudflare to localhost:30080
  https:
    enabled: false  # Cloudflare handles TLS

hub:
  service:
    type: ClusterIP
  db:
    type: sqlite-pvc  # Simple SQLite database
  config:
    JupyterHub:
      admin_access: true
  
  # ============================================
  # AZURE AD CREDENTIALS FROM KUBERNETES SECRET
  # ============================================
  # Credentials are stored securely in Kubernetes secret.
  # 
  # To create the secret, run on your NVIDIA server:
  #   cd helm && ./create-azure-secret.sh
  #
  # This loads environment variables from the secret:
  # ============================================
  extraEnv:
    AZURE_TENANT_ID:
      valueFrom:
        secretKeyRef:
          name: jupyterhub-azure-oauth
          key: tenant-id
    AZURE_CLIENT_ID:
      valueFrom:
        secretKeyRef:
          name: jupyterhub-azure-oauth
          key: client-id
    AZURE_CLIENT_SECRET:
      valueFrom:
        secretKeyRef:
          name: jupyterhub-azure-oauth
          key: client-secret

  extraConfig:
    # ============================================
    # AZURE AD / ENTRA ID AUTHENTICATION
    # ============================================
    # Credentials are loaded from Kubernetes secret (see extraEnv above).
    #
    # First-time setup:
    # 1. Create the secret: cd helm && ./create-azure-secret.sh
    # 2. Deploy JupyterHub: ./deploy_jhub_helm.sh
    #
    # To update credentials:
    # 1. Delete old secret: kubectl delete secret jupyterhub-azure-oauth -n jupyterhub-test
    # 2. Re-run: ./create-azure-secret.sh
    # 3. Restart hub: kubectl rollout restart deployment hub -n jupyterhub-test
    # ============================================
    00-azure-ad-auth: |
      import os
      from oauthenticator.azuread import AzureAdOAuthenticator
      c.JupyterHub.authenticator_class = AzureAdOAuthenticator
      
      # ============================================
      # CREDENTIALS FROM KUBERNETES SECRET
      # ============================================
      # These are loaded from environment variables (set via extraEnv above)
      # The actual values are stored in Kubernetes secret: jupyterhub-azure-oauth
      # ============================================
      
      c.AzureAdOAuthenticator.tenant_id = os.environ.get('AZURE_TENANT_ID')
      c.AzureAdOAuthenticator.client_id = os.environ.get('AZURE_CLIENT_ID')
      c.AzureAdOAuthenticator.client_secret = os.environ.get('AZURE_CLIENT_SECRET')
      c.AzureAdOAuthenticator.oauth_callback_url = "https://jupyterhub.ccrolabs.com/hub/oauth_callback"
      
      # ============================================
      # GROUP-BASED ACCESS CONTROL (RECOMMENDED)
      # ============================================
      # Azure AD sends group Object IDs (GUIDs), NOT group names!
      #
      # To get your group Object IDs:
      # 1. Azure Portal ‚Üí Microsoft Entra ID ‚Üí Groups
      # 2. Click on "JupyterHub-Users" ‚Üí Properties ‚Üí Copy "Object ID"
      # 3. Click on "JupyterHub-Admins" ‚Üí Properties ‚Üí Copy "Object ID"
      # 4. Replace the example UUIDs below with your actual Object IDs
      #
      # Also configure Azure AD app to include groups in token:
      # Azure Portal ‚Üí App registrations ‚Üí JupyterHub ‚Üí Token configuration
      # ‚Üí Add groups claim ‚Üí Select "Security groups"
      # ============================================
      
      # Enable group management
      c.AzureAdOAuthenticator.manage_groups = True
      c.Authenticator.manage_groups = True
      
      # Tell JupyterHub where to find groups in the token
      c.AzureAdOAuthenticator.claim_groups_key = "groups"
      
      # ============================================
      # OPTION A: Use Group Object IDs (GUIDs) - RECOMMENDED
      # ============================================
      # Replace these with YOUR actual group Object IDs from Azure AD!
      # Get them from: Azure AD ‚Üí Groups ‚Üí [Group] ‚Üí Properties ‚Üí Object ID
      
      # Example Object IDs (REPLACE WITH YOUR ACTUAL IDs!):
      # JUPYTERHUB_USERS_GROUP_ID = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      # JUPYTERHUB_ADMINS_GROUP_ID = "yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy"
      
      c.AzureAdOAuthenticator.allowed_groups = {
          # Replace with your JupyterHub-Users Object ID
          "6543851f-fd97-40e8-b097-ab5a71e44ef2",
          # Replace with your JupyterHub-Admins Object ID  
          "d6c49ed5-eefc-48c4-90d0-2026f5fe3916"
      }
      
      c.AzureAdOAuthenticator.admin_groups = {
          # Replace with your JupyterHub-Admins Object ID
          "d6c49ed5-eefc-48c4-90d0-2026f5fe3916"
      }
      
      # ============================================
      # OPTION B: Use Group Names (requires Azure AD Premium)
      # ============================================
      # If you have Azure AD Premium, you can configure Azure AD
      # to send group names instead of IDs. Uncomment below:
      #
      # c.AzureAdOAuthenticator.allowed_groups = {
      #     "JupyterHub-Users",
      #     "JupyterHub-Admins"
      # }
      # c.AzureAdOAuthenticator.admin_groups = {
      #     "JupyterHub-Admins"
      # }
      
      
      # ============================================
      # ALTERNATIVE: Email-Based Access Control (Not Recommended)
      # ============================================
      # If you prefer email-based control instead of groups, comment out the
      # allowed_groups and admin_groups above, and uncomment these:
      #
      # c.Authenticator.allowed_users = {
      #     "user1@yourcompany.com",
      #     "user2@yourcompany.com"
      # }
      # c.Authenticator.admin_users = {
      #     "admin@yourcompany.com"
      # }
      #
      # NOTE: Requires JupyterHub redeploy to add/remove users!
      # ============================================

    # ============================================
    # ALTERNATIVE: GitHub OAuth (Commented out)
    # ============================================
    # 00-github-auth: |
    #   from oauthenticator.github import GitHubOAuthenticator
    #   c.JupyterHub.authenticator_class = GitHubOAuthenticator
    #   
    #   c.GitHubOAuthenticator.client_id = "Ov23lidtVx7ofQPm6Qvz"
    #   c.GitHubOAuthenticator.client_secret = "ef282ca998be6a552b00f3ccfe62f18f195a9eae"
    #   c.GitHubOAuthenticator.oauth_callback_url = "https://jupyterhub.ccrolabs.com/hub/oauth_callback"
    #   c.Authenticator.allow_all = True
    #   c.Authenticator.admin_users = {"your-github-username"}
    
    # ============================================
    # ALTERNATIVE: No Authentication (Testing only)
    # ============================================
    # 00-simple-auth: |
    #   from jupyterhub.auth import DummyAuthenticator
    #   c.JupyterHub.authenticator_class = DummyAuthenticator
    #   c.DummyAuthenticator.password = ""
    #   c.Authenticator.admin_users = {"admin"}

singleuser:
  image:
    # Official JupyterHub base with GPU libraries
    name: docker.io/ermircjapi/jupyterhub-notebook
    tag: v2.3  # PyTorch nightly for RTX 5090 support (sm_120)
    pullPolicy: Always
  
  storage:
    type: dynamic
    capacity: 10Gi
    dynamic:
      storageClass: local-path  # k3s creates storage automatically
  
  # Start in JupyterLab
  defaultUrl: /lab
  
  # Fix PVC permissions using fsGroup
  extraPodConfig:
    securityContext:
      fsGroup: 100
      fsGroupChangePolicy: "OnRootMismatch"
  
  # Disable cloud metadata blocking (requires privileged init container)
  cloudMetadata:
    blockWithIptables: false
  
  # ============================================
  # USER PROFILES - Choose CPU or GPU at login
  # ============================================
  # Users select profile when logging in:
  # - CPU Only: Always available, for data exploration/light work
  # - GPU Enabled: Limited to 8 concurrent users (with time-slicing)
  # ============================================
  
  profileList:
    - display_name: "üíª CPU Only - Data Exploration"
      description: |
        üöÄ Always available (no waiting)
        üìä Perfect for: data analysis, visualization, pandas/numpy
        ‚ö° 2 CPUs, 4GB RAM
        ‚ùå No GPU access
      default: true
      kubespawner_override:
        cpu_limit: 2
        cpu_guarantee: 0.5
        mem_limit: "4G"
        mem_guarantee: "1G"
        # No GPU resources
    
    - display_name: "üéÆ GPU Enabled - ML/AI Training"
      description: |
        üî• 1 GPU slice (RTX 5090)
        ü§ñ Perfect for: PyTorch, TensorFlow, deep learning
        ‚ö° 4 CPUs, 8GB RAM, 1 GPU
        ‚è≥ May queue if 8 users active
      kubespawner_override:
        cpu_limit: 4
        cpu_guarantee: 1
        mem_limit: "8G"
        mem_guarantee: "2G"
        extra_resource_limits:
          nvidia.com/gpu: "1"
        extra_resource_guarantees:
          nvidia.com/gpu: "1"
  
  # ============================================
  # GPU TIME-SLICING CONFIGURATION
  # ============================================
  # IMPORTANT: GPU Time-Slicing must be configured first!
  # See: GPU-SHARING.md for details
  # 
  # With time-slicing enabled (replicas=8):
  # - 1 physical GPU becomes 8 virtual GPUs
  # - 8 users can work simultaneously on GPU profile
  # - Each gets ~12.5% when all active, 100% when alone
  # - Unlimited users on CPU-only profile
  #
  # Without time-slicing (default):
  # - Only 1 user at a time per physical GPU
  # - Other users wait in queue (Pending pods)
  #
  # To enable time-slicing on your cluster (RUN ONCE):
  #   ./setup-gpu-timeslicing.sh
  # ============================================

# Auto-shutdown idle notebooks
cull:
  enabled: true
  timeout: 3600   # 1 hour
  every: 600      # check every 10 minutes

# Keep image pulled
prePuller:
  continuous:
    enabled: true

# Disable user-scheduler (use default K3s scheduler)
scheduling:
  userScheduler:
    enabled: false
