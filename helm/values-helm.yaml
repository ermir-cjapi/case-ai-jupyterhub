# JupyterHub Helm Values - Kubernetes Deployment
# This is the same as jupyterhub_config.py but in YAML format for Kubernetes

proxy:
  # Generate with: openssl rand -hex 32
  secretToken: "a7c63b7d74e9628282eca95874fa7a9f69797f027512bb6d9f0636bb9689307e"
  service:
    type: NodePort  # Expose on fixed port for Cloudflare tunnel
    nodePorts:
      http: 30080  # Fixed port - update Cloudflare to localhost:30080
  https:
    enabled: false  # Cloudflare handles TLS

hub:
  service:
    type: ClusterIP
  db:
    type: sqlite-pvc  # Simple SQLite database
  config:
    JupyterHub:
      admin_access: true
  
  extraConfig:
    # ============================================
    # GITHUB OAUTH AUTHENTICATION
    # ============================================
    # Setup: Create OAuth App at https://github.com/settings/developers
    # 1. Application name: JupyterHub CCRO Labs
    # 2. Homepage URL: https://jupyterhub.ccrolabs.com
    # 3. Authorization callback URL: https://jupyterhub.ccrolabs.com/hub/oauth_callback
    # 4. Copy Client ID and generate Client Secret
    # 5. Update values below
    00-github-auth: |
      from oauthenticator.github import GitHubOAuthenticator
      c.JupyterHub.authenticator_class = GitHubOAuthenticator
      
      # REPLACE WITH YOUR GITHUB OAUTH APP CREDENTIALS:
      c.GitHubOAuthenticator.client_id = "Ov23lidtVx7ofQPm6Qvz"
      c.GitHubOAuthenticator.client_secret = "ef282ca998be6a552b00f3ccfe62f18f195a9eae"
      c.GitHubOAuthenticator.oauth_callback_url = "https://jupyterhub.ccrolabs.com/hub/oauth_callback"
      
      # Access control:
      # Option 1: Allow specific GitHub users
      # c.GitHubOAuthenticator.allowed_users = {"username1", "username2"}
      
      # Option 2: Allow all users who can authenticate (any GitHub user)
      c.Authenticator.allow_all = True
      
      # Admin users (by GitHub username)
      c.Authenticator.admin_users = {"your-github-username"}

    # ============================================
    # ALTERNATIVE: Azure AD / Entra ID (Commented out)
    # ============================================
    # 00-azure-ad-auth: |
    #   from oauthenticator.azuread import AzureAdOAuthenticator
    #   c.JupyterHub.authenticator_class = AzureAdOAuthenticator
    #   
    #   # REPLACE WITH YOUR AZURE AD CREDENTIALS:
    #   c.AzureAdOAuthenticator.tenant_id = "YOUR_TENANT_ID"
    #   c.AzureAdOAuthenticator.client_id = "YOUR_CLIENT_ID"
    #   c.AzureAdOAuthenticator.client_secret = "YOUR_CLIENT_SECRET"
    #   c.AzureAdOAuthenticator.oauth_callback_url = "https://jupyterhub.ccrolabs.com/hub/oauth_callback"
    #   
    #   # Access control
    #   c.Authenticator.allow_all = True
    #   c.Authenticator.admin_users = {"your-email@yourcompany.com"}
    
    # ============================================
    # ALTERNATIVE: No Authentication (Testing only)
    # ============================================
    # 00-simple-auth: |
    #   from jupyterhub.auth import DummyAuthenticator
    #   c.JupyterHub.authenticator_class = DummyAuthenticator
    #   c.DummyAuthenticator.password = ""
    #   c.Authenticator.admin_users = {"admin"}

singleuser:
  image:
    # Official JupyterHub base with GPU libraries and fixed permissions
    name: docker.io/ermircjapi/jupyterhub-notebook
    tag: v2.1  # Stable working version with GPU sharing
    pullPolicy: Always
  
  storage:
    type: dynamic
    capacity: 10Gi
    dynamic:
      storageClass: local-path  # k3s creates storage automatically
  
  # Fix PVC permissions with pod security context
  podSecurityContext:
    fsGroup: 100
    fsGroupChangePolicy: "OnRootMismatch"
  
  # Resource limits per user
  cpu:
    limit: 2
    guarantee: 0.5
  memory:
    limit: 4G
    guarantee: 1G
  
  # Start in JupyterLab
  defaultUrl: /lab
  
  # Disable cloud metadata blocking (requires privileged init container)
  cloudMetadata:
    blockWithIptables: false
  
  # ============================================
  # GPU CONFIGURATION
  # ============================================
  # IMPORTANT: GPU Time-Slicing must be configured first!
  # See: GPU-SHARING.md for details
  # 
  # With time-slicing enabled (replicas=4):
  # - 1 physical GPU becomes 4 virtual GPUs
  # - 4 users can work simultaneously
  # - Each gets ~25% when all active, 100% when alone
  #
  # Without time-slicing (default):
  # - Only 1 user at a time per physical GPU
  # - Other users wait in queue (Pending pods)
  #
  # To enable time-slicing on your cluster (RUN ONCE):
  #   ./setup-gpu-timeslicing.sh
  # ============================================
  
  extraResource:
    limits:
      nvidia.com/gpu: "1"  # Each user gets 1 GPU slice (1/4 of RTX 5090 if time-slicing enabled)
    guarantees:
      nvidia.com/gpu: "1"

# Auto-shutdown idle notebooks
cull:
  enabled: true
  timeout: 3600   # 1 hour
  every: 600      # check every 10 minutes

# Keep image pulled
prePuller:
  continuous:
    enabled: true

# Disable user-scheduler (use default K3s scheduler)
scheduling:
  userScheduler:
    enabled: false
